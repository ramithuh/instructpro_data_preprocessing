# instructpro_data_preprocessing

This repo documents the steps taken to produce the instruct_pro dataset, starting from the raw data files from UniprotKB. If you have any questions, please feel free to reach out to me at ruh@cs.cmu.edu

### Step 0: Downlaod the raw data

Download these files from the following links:
```bash
wget https://static.ramith.io/scientificLLM/data/ligand2smiles.json
wget https://static.ramith.io/scientificLLM/data/protein2ligand_id.json
wget https://static.ramith.io/scientificLLM/data/uniprot2seq.json
wget https://static.ramith.io/scientificLLM/data/uniprot2text.json
wget https://static.ramith.io/scientificLLM/data/uniref50.jsonl
```

### Step 1: Run the Analysis script to see the number of uniprot IDs that have all three: sequence, text, and associated ligands

Running this code (`python 1.\ intersection_curation.py`) will also save the uniprot IDs that have all three: sequence, text, and associated ligands to a file called `selected_uniprot_ids.txt`. This file will be later used for mmseqs2 to generate the clusters.

<details>
  <summary>Show expected output</summary>

```bash
(py311) ramith@taurus:~/instructpro_data_preprocessing$ python 1.\ intersection_curation.py 
Protein to Ligand IDs: 12235657
UniProt to Sequence: 33617397
UniProt to Text: 35584366
Ligand, Sequence, and Text intersection: 7680607
Sequence and Text intersection: 33617397
Ligand and Text intersection: 7997705
All elements of seq appear in text
Elements in ligand but not in text: 4237952
Elements in ligand and text but not in seq: 317098
Uniref50 keys: 64363428
Biggest intersection after adding Uniref50: 273568
Regular Intersection: 7680607
Intersection with Uniref50: 273568
Total uniprot ids after adding Uniref50: 7718246
IDs have been saved to selected_uniprot_ids.txt
```
</details>

### Step 2: Generate a fasta file for the selected uniprot IDs and run mmseqs2 to generate clusters

Run the following python script (`python 2.\ make_fasta.py`) to generate a fasta file.

<details>
  <summary>Show expected output</summary>

```bash
(py311) ramith@taurus:~/instructpro_data_preprocessing$ python 2.\ make_fasta.py 
7680607 written from uniprot2seq.json
37639 written from uniref50.jsonl
0
```
</details>
<br>

Then run this to generate the clusters using mmseqs2: keep an eye out for the clusterRes_cluster.tsv which maps each protein sequence ID to a cluster ID. In our run, this resulted in 22,594 clusters [(mmseqs output)](https://static.ramith.io/scientificLLM/mmseqs2_attemp2.txt).

```bash
mmseqs easy-cluster output_sequences.fasta clusterRes tmp --min-seq-id 0.3 -c 0.8 --cov-mode 1
```

### Step 3: Split the clusters into train, validation, and test sets

This step uses the script `3. split_sets.py` to divide the ~22,594 protein clusters (obtained from MMseqs2 in Step 3) into training, validation, and test sets. The primary input for this script is the `clusterRes_cluster.tsv` file generated by MMseqs2.

The script performs the following key operations:
1.  **Calculates Cluster Sizes:** It reads the cluster content from `clusterRes_cluster.tsv` and determines the number of protein sequences within each cluster.

2.  **Stratified Sampling for Validation/Test:** To ensure a representative mix of cluster sizes in the valid and test sets, it employs a stratified sampling strategy. Clusters are binned by size (e.g., 1-500 sequences, 501-1000 sequences, etc.), and a predefined number of clusters are randomly sampled from each bin. For instance, 40 clusters are sampled from the 1-500 sequence count range, 20 from 501-1000, 10 from 1001-2500, and 2 from clusters with over 2500 sequences.

3.  **Assigns to Validation and Test:** This total pool of 72 sampled clusters is then divided equally (36 clusters each) to form the validation and test sets. This ensures each of these sets receives representation from the different cluster size bins (e.g., ~20 from the 1-500 bin, ~10 from the 501-1000 bin, etc., per set).

4.  **Assigns to Training:** All clusters not selected for either validation or test are assigned to the training set.

5.  **Outputs:** The script saves the lists of cluster IDs for each set into three separate files:
    * `train_clusters.txt`
    * `val_clusters.txt`
    * `test_clusters.txt`
    It also generates an HTML file, `cluster_histogram.html`, visualizing the distribution of sequence counts per cluster.

(script uses a fixed random seed (`42`) to ensure these splits are reproducible)

### Step 4: Designate Specific Ligands for Validation and Test Sets

This step uses the script `4. split_ligands.py` to define distinct sets of ligands that will be considered "unseen" during training. This is crucial for evaluating the model's ability to generalize to novel chemical compounds.

The script performs a refined selection process:

1.  **Initial Ligand Consideration:** It analyzes the distribution of all ligands associated with the 7.7 million selected proteins.
2.  **Integration with Protein Clusters:** It loads the previously defined validation and test protein cluster assignments (`val_clusters.txt`, `test_clusters.txt`) and identifies the actual ligands present within the proteins belonging to these clusters.
3.  **Refinement and Disambiguation:** An initial sampling of ligands is cross-referenced with the ligands found in the validation and test protein clusters. Any ligands that would ambiguously appear in both the refined validation and test ligand sets are removed to ensure strict separation.
4.  **Outputs:** The script saves the final, distinct lists of ligand IDs into two files:
    * `ligands_val.txt` (containing 9 unique ligands)
    * `ligands_test.txt` (containing 5 unique ligands)

### Step 5: Generate Final Tokenized Datasets

This crucial step uses the script `5. generate_tokenized_datasets.py` to assemble the final datasets ready for model training and evaluation. It combines the protein cluster splits (from `train_clusters.txt`, `val_clusters.txt`, `test_clusters.txt`), designated ligand sets (from `ligands_val.txt`, `ligands_test.txt`), and the raw protein information (sequence, function, ligand SMILES). A key feature of this step is the tokenization of protein functional descriptions using a BioBERT tokenizer, with custom `<FUNCTION>` tags prepended and appended to the functional text.

**Key Operations & Outputs:**

The script generates five distinct datasets in JSONL format. Each line in these files is a record containing the original function text, tokenized function (as string tokens and as numerical IDs), ligand SMILES, ligand ID, the target protein sequence, and the UniProt ID. The generated files are:

1.  **`train_dataset_tokenized.jsonl`**: Contains protein-ligand-function triplets from the training protein clusters. Ligands specifically designated for the validation and test sets (from Step 4) are excluded from these pairings.
2.  **`val_dataset_tokenized.jsonl`**: Designed for evaluating generalization to *unseen* ligands. Contains triplets from validation protein clusters, exclusively using ligands from `ligands_val.txt`.
3.  **`test.dataset_both_unseen_tokenized.jsonl`**: Also for evaluating generalization to *unseen* ligands. Contains triplets from test protein clusters, exclusively using ligands from `ligands_test.txt`.
4.  **`val_dataset_seen_ligands_tokenized.jsonl`**: For evaluating model performance on ligands that were *seen* during training but within the context of validation protein clusters. Contains triplets from validation protein clusters, using only ligands that were part of the training ligand pool (i.e., not in `ligands_val.txt` or `ligands_test.txt`).
5.  **`test.dataset_seen_ligands_tokenized.jsonl`**: Similar to the above, but for test protein clusters using training pool ligands.

### Final Remarks:

Note that since the `test_dataset_seen_ligands_tokenized.jsonl` is very huge (19436 examples), we sampled 1500 examples from it for testing.